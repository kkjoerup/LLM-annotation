{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import transformers\n",
    "import re\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "from huggingface_hub import interpreter_login\n",
    "from sklearn.utils import resample\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acknowledgment\n",
    "The code is adapted from unlslothai's work available at https://github.com/unslothai/unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate confidence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all remaining data (data not included in prompt selection or prompting experiment): text, label\n",
    "data = pd.read_csv(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "# Authenticate with Huggingface token\n",
    "#!git config --global credential.helper store\n",
    "interpreter_login()\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\" \n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt for confidence scores\n",
    "sentiment_v3_prompt_confidence = \"\"\"\n",
    "Du er en gennemsnitlig dansk nyhedsforbruger. Du får en overskrift og underoverskrift på en artikel, og skal tildele den en kategori svarende til det sentiment den fremkalder. \n",
    "Her er nogle generelle principper, du skal følge:\n",
    "Du skal bruge din viden om det danske samfund og annotere artiklen som du forestiller dig, at den gennemsnitlige dansker ville gøre det.\n",
    "Undgå at lade dig påvirke af personlige holdninger og bias. Nogle artikler udtrykker holdninger som du måske er uenig med, men det må ikke påvirke vurderingen af artiklens sentiment.\n",
    "Du skal tildele artiklen dens mest dominerende sentiment, hvis den vurderes at indeholde flere sentiments.\n",
    "Der findes både explicit og implicit sentiment. Explicit sentiment afspejler ofte nogens indre tilstand (f.eks. tro, holdninger, tanker, følelser osv.). Implicit sentiment afspejler derimod ofte en kendsgerning, der fører til et positivt eller negativt sentiment omkring et emne (f.eks. en person eller begivenhed). Artikler fremkalder ofte implicit sentiment ved at fremhæve gode eller dårlige begivenheder. F.eks. er begivenheder som fødsler, at blive gift eller forfremmet gode begivenheder, mens f.eks. død og sygdom er dårlige begivenheder.\n",
    "Du skal bruge din generelle viden om følelsesladede udtryk på dansk. Vær opmærksom på, at sentiment kan forekomme ved ord, der normalt ikke betragtes som følelsesladede, f.eks. familie, løn og ansættelse. Nogle begivenheder er også forbundet med sentiment, f.eks. 1. verdenskrig eller Covid-19.\n",
    "Kategorier: ”Positiv”: Fremkalder en overordnet positiv sentiment. Stemninger som optimisme, tilfredshed og selvsikkerhed betragtes som positive. ”Negativ”: Fremkalder en overordnet negativ sentiment. Stemninger som vrede, skuffelse og tristhed betragtes som negative. ”Neutral”: Fremkalder hverken en positiv eller negativ sentiment. Enten ingen sentiment eller tvetydig sentiment. \n",
    "Giv også en confidence score med to decimaler fra 0.00 til 1.00, der repræsenterer hvor sikker du er i din vurdering af sentiment, hvor 0 er meget usikker og 1 er meget sikker.\n",
    "Giv et præcist svar i json: {{sentiment: ”kategori”, \"confidence\": \"score\"}}.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run Llama with few-shot prompts\n",
    "def fewshot_sentiment_annotation(text, prompt, fewshot_dataset):\n",
    "    # Sample few-shot examples from dataset without current article\n",
    "    fewshot_dataset = fewshot_dataset[fewshot_dataset[\"text\"] != text]\n",
    "    fewshot_examples = fewshot_dataset.sample(3).to_dict(orient=\"records\")\n",
    "    # Create few-shot part of prompt\n",
    "    fewshot_examples = \"\\n\".join([f\"Artikel: {example[\"text\"]}. Artiklen fremkalder dette sentiment: {example[\"label\"]}.\" for example in fewshot_examples])\n",
    "    \n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": f\"{prompt}. Her er tre eksempler på artikler og deres sentiment: {fewshot_examples}\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Artikel: {text}. Artiklen fremkalder dette sentiment: \"},\n",
    "    ]\n",
    "\n",
    "    outputs = pipeline(\n",
    "        messages,\n",
    "        max_new_tokens=256,\n",
    "    )\n",
    "    \n",
    "    # Return the generated content\n",
    "    return outputs[0][\"generated_text\"][-1][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply few-shot\n",
    "data[\"llm_annotation\"] = data[\"text\"].apply(lambda text: fewshot_sentiment_annotation(text, prompt=sentiment_v3_prompt_confidence, fewshot_dataset=data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract confidence scores\n",
    "def extract_confidence(value):\n",
    "    match = re.search(r'\"confidence\":\\s*\"?(\\d+\\.\\d+)\"?', str(value))\n",
    "    if match:\n",
    "        return int(float(match.group(1)))\n",
    "    return None  # Return None if no match is found\n",
    "\n",
    "data[\"confidence_scores\"] = data[\"llm_annotation\"].apply(extract_confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map labels to integers\n",
    "def map_sentiment_to_int(annotation):\n",
    "    if \"Negativ\" in annotation or \"negativ\" in annotation:\n",
    "        return 0\n",
    "    elif \"Neutral\" in annotation or \"neutral\" in annotation:\n",
    "        return 1\n",
    "    elif \"Positiv\" in annotation or \"positiv\" in annotation:\n",
    "        return 2\n",
    "    else:\n",
    "        return -1 \n",
    "\n",
    "data[\"label\"] = data[\"label\"].map(map_sentiment_to_int)\n",
    "data[\"llm_annotation\"] = data[\"llm_annotation\"].map(map_sentiment_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(n=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selective sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bins and labels for confidence intervals\n",
    "bins = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "labels = [\"0-0.1\", \"0.1-0.2\", \"0.2-0.3\", \"0.3-0.4\", \"0.4-0.5\", \"0.5-0.6\", \"0.6-0.7\", \"0.7-0.8\", \"0.8-0.9\", \"0.9-1.0\"]\n",
    "data[\"confidence_interval\"] = pd.cut(data[\"confidence_scores\"], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "# Determine if LLM annotation is correct\n",
    "data[\"is_correct\"] = data[\"llm_annotation\"] == data[\"label\"]\n",
    "\n",
    "# Calculate the distribution of examples across intervals and correctness in the full dataset\n",
    "full_distribution = data.groupby([\"confidence_interval\", \"is_correct\"]).size() / len(data)\n",
    "\n",
    "# Calculate target sample sizes based on this distribution\n",
    "total_samples = 300\n",
    "sample_sizes = (full_distribution * total_samples).round().astype(int)\n",
    "\n",
    "# Initialize an empty DataFrame for the sampled data\n",
    "sampled_df = pd.DataFrame()\n",
    "\n",
    "# Sample 300 examples based on the calculated distribution\n",
    "for (interval, correct), size in sample_sizes.items():\n",
    "    # Only sample if the size is greater than zero\n",
    "    if size > 0:\n",
    "        # Get the subset of data for this interval and correctness\n",
    "        group_data = data[(data[\"confidence_interval\"] == interval) & (data[\"is_correct\"] == correct)]\n",
    "        \n",
    "        # Set `replace=True` only if we need more samples than available in this group\n",
    "        replace = size > len(group_data)\n",
    "        \n",
    "        # Sample the data\n",
    "        sampled_group = resample(\n",
    "            group_data,\n",
    "            n_samples=size,\n",
    "            random_state=42,\n",
    "            replace=replace\n",
    "        )\n",
    "        sampled_df = pd.concat([sampled_df, sampled_group])\n",
    "\n",
    "# Reset index for the sampled DataFrame\n",
    "sampled_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "data = sampled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 5000 \n",
    "dtype = None \n",
    "load_in_4bit = True\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    token = os.getenv(\"HUGGINGFACE_TOKEN\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = True, \n",
    "    loftq_config = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format fine-tuning data to chat template\n",
    "data[\"text\"] = data[\"messages\"].apply(lambda chat: tokenizer.apply_chat_template(chat, tokenize=False))\n",
    "\n",
    "# convert to Huggingface dataset\n",
    "data_dict = {\"text\": data[\"text\"].tolist()}\n",
    "dataset = Dataset.from_dict(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 8,\n",
    "        warmup_steps = 10,\n",
    "        max_steps = 100,\n",
    "        learning_rate = 1e-5, \n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 5,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation data\n",
    "evaluation_data = pd.read_csv(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V0 zeroshot \n",
    "def run_v0_sentiment_inference(text, model):\n",
    "    # Define the chat input, inserting the text\n",
    "    chat_input = [\n",
    "        f'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n'\n",
    "        f'Cutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\n'\n",
    "        f'Du er en gennemsnitlig dansk nyhedsforbruger. Du får en overskrift og underoverskrift'\n",
    "        f'på en artikel, og skal tildele den en kategori svarende til det sentiment den fremkalder.'\n",
    "        f'Kategorier: ”Positiv”, ”Negativ”, ”Neutral”'\n",
    "        f'Giv et præcist svar i json: {{sentiment: ”kategori”}}.<|eot_id|><|start_header_id|>user<|end_header_id>\\n\\n'\n",
    "        f'Artikel: {text} \\nArtiklen fremkalder dette sentiment:<|eot_id|><|start_header_id|>assistant<|end_header_id>\\n\\n'\n",
    "    ]\n",
    "\n",
    "    # Tokenize the chat input for the model\n",
    "    inputs = tokenizer(chat_input, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Generate the output\n",
    "    outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\n",
    "\n",
    "    # Decode the generated output\n",
    "    decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return decoded_outputs\n",
    "\n",
    "evaluation_data[\"v0-zeroshot-annotation\"] = evaluation_data[\"text\"].apply(lambda text: run_v0_sentiment_inference(text, model=model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '3.11.1 (Python 3.11.1)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/katrine.k.jensen/.pyenv/versions/3.11.1/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# V3 few-shot\n",
    "def run_v3_fewshot_sentiment_inference(text, model, fewshot_dataset):\n",
    "    # Sample few-shot examples from dataset without current article\n",
    "    fewshot_dataset = fewshot_dataset[fewshot_dataset[\"text\"] != text]\n",
    "    fewshot_examples = fewshot_dataset.sample(1).to_dict(orient=\"records\")\n",
    "    # Create few-shot part of prompt\n",
    "    fewshot_example = \"\\n\".join([f\"Artikel: {example['text']}. Artiklen fremkalder dette sentiment: {example['label']}.\" for example in fewshot_examples])\n",
    "    \n",
    "    # Define the chat input, inserting the text. v3 prompt    \n",
    "    chat_input = [\n",
    "        f'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n'\n",
    "        f'Cutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\n'\n",
    "        f'Du er en gennemsnitlig dansk nyhedsforbruger. Du får en overskrift og underoverskrift på en artikel, og skal tildele den en kategori svarende til det sentiment den fremkalder. Her er nogle generelle principper, du skal følge: Du skal bruge din viden om det danske samfund og annotere artiklen som du forestiller dig, at den gennemsnitlige dansker ville gøre det. Undgå at lade dig påvirke af personlige holdninger og bias. Nogle artikler udtrykker holdninger som du måske er uenig med, men det må ikke påvirke vurderingen af artiklens sentiment. Du skal tildele artiklen dens mest dominerende sentiment, hvis den vurderes at indeholde flere sentiments. Der findes både explicit og implicit sentiment. Explicit sentiment afspejler ofte nogens indre tilstand (f.eks. tro, holdninger, tanker, følelser osv.). Implicit sentiment afspejler derimod ofte en kendsgerning, der fører til et positivt eller negativt sentiment omkring et emne (f.eks. en person eller begivenhed). Artikler fremkalder ofte implicit sentiment ved at fremhæve gode eller dårlige begivenheder. F.eks. er begivenheder som fødsler, at blive gift eller forfremmet gode begivenheder, mens f.eks. død og sygdom er dårlige begivenheder. Du skal bruge din generelle viden om følelsesladede udtryk på dansk. Vær opmærksom på, at sentiment kan forekomme ved ord, der normalt ikke betragtes som følelsesladede, f.eks. familie, løn og ansættelse. Nogle begivenheder er også forbundet med sentiment, f.eks. 1. verdenskrig eller Covid-19. '\n",
    "        f'Kategorier: ”Positiv”: Fremkalder en overordnet positiv sentiment. Stemninger som optimisme, tilfredshed og selvsikkerhed betragtes som positive. ”Negativ”: Fremkalder en overordnet negativ sentiment. Stemninger som vrede, skuffelse og tristhed betragtes som negative. ”Neutral”: Fremkalder hverken en positiv eller negativ sentiment. Enten ingen sentiment eller tvetydig sentiment. '\n",
    "        f'Her er eksempler på en artikler og sentiment: {fewshot_example}'\n",
    "        f'Giv et præcist svar i json: {{sentiment: ”kategori”}}.<|eot_id|><|start_header_id|>user<|end_header_id>\\n\\n'\n",
    "        f'Artikel: {text} \\nArtiklen fremkalder dette sentiment:<|eot_id|><|start_header_id|>assistant<|end_header_id>\\n\\n'\n",
    "    ]\n",
    "\n",
    "    # Tokenize the chat input for the model\n",
    "    inputs = tokenizer(chat_input, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Generate the output\n",
    "    outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\n",
    "\n",
    "    # Decode the generated output\n",
    "    decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return decoded_outputs\n",
    "\n",
    "evaluation_data[\"v3-fewshot-annotation\"] = evaluation_data[\"text\"].apply(lambda text: run_v3_fewshot_sentiment_inference(text, model=model, fewshot_dataset=evaluation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additionally, extract [-30:]\n",
    "evaluation_data[\"v3-fewshot-llm_annotation\"] = evaluation_data[\"v3-fewshot-annotation\"].apply(lambda text: text[0][-30:])\n",
    "evaluation_data[\"v0-fewshot-llm_annotation\"] = evaluation_data[\"v0-zeroshot-annotation\"].apply(lambda text: text[0][-30:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save evaluation results in csv\n",
    "evaluation_data.to_csv(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
