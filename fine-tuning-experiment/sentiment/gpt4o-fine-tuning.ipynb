{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.utils import resample\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#API key\n",
    "client = OpenAI(api_key = os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all remaining data (data not included in prompt selection or prompting experiment): text, label\n",
    "data = pd.read_csv(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate confidence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt for confidence scores\n",
    "sentiment_v1_prompt_confidence = \"\"\"\n",
    "Du er en gennemsnitlig dansk nyhedsforbruger. Du får en overskrift og underoverskrift på en artikel, og skal tildele den en kategori svarende til det sentiment den fremkalder.\n",
    "Kategorier: ”Positiv”: Fremkalder en overordnet positiv sentiment. ”Negativ”: Fremkalder en overordnet negativ sentiment. ”Neutral”: Fremkalder hverken en positiv eller negativ sentiment\n",
    "Giv også en confidence score med to decimaler fra 0.00 til 1.00, der repræsenterer hvor sikker du er i din vurdering af sentiment, hvor 0 er meget usikker og 1 er meget sikker.\n",
    "Giv et præcist svar i json: {{sentiment: ”kategori”, \"confidence\": \"score\"}}.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to call GPT-4o with zero-shot prompts. Return json\n",
    "def zeroshot_sentiment_annotation(text, prompt):\n",
    "    try:\n",
    "        # Make a request to GPT-4o\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-2024-08-06\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\", \n",
    "                    \"content\": f\"{prompt}\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": f\"Artikel: {text} \\nArtiklen fremkalder dette sentiment:\"\n",
    "                }\n",
    "            ],\n",
    "            temperature=0,\n",
    "            response_format={\n",
    "                \"type\": \"json_schema\",\n",
    "                \"json_schema\": {\n",
    "                    \"name\": \"topic_schema\",\n",
    "                    \"schema\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"sentiment\": {\n",
    "                                \"description\": \"Sentiment of the article\",\n",
    "                                \"type\": \"string\"\n",
    "                            },\n",
    "                            \"confidence\": {\n",
    "                                \"description\": \"Confidence score for the sentiment from 0.00 to 1.00\",\n",
    "                                \"type\": \"number\",\n",
    "                                \"minimum\": 0.0,\n",
    "                                \"maximum\": 1.0\n",
    "                            }\n",
    "                        },\n",
    "                        \"additionalProperties\": False\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Extract response\n",
    "        sentiment_data = response.choices[0].message.content\n",
    "        return sentiment_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "# Apply function and add results to df\n",
    "data[\"llm_annotation\"] = data[\"text\"].apply(lambda text: zeroshot_sentiment_annotation(text, prompt=sentiment_v1_prompt_confidence))\n",
    "\n",
    "# Save the results to a new CSV file\n",
    "data.to_csv(\"\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract confidence scores\n",
    "def extract_confidence(value):\n",
    "    match = re.search(r'\"confidence\":\\s*\"?(\\d+\\.\\d+)\"?', str(value))\n",
    "    if match:\n",
    "        return int(float(match.group(1)))\n",
    "    return None  # Return None if no match is found\n",
    "\n",
    "data[\"confidence_scores\"] = data[\"llm_annotation\"].apply(extract_confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map labels to integers\n",
    "def map_sentiment_to_int(annotation):\n",
    "    # LLM output is not consistent, therefore search for string match in output\n",
    "    if \"Negativ\" in annotation or \"negativ\" in annotation:\n",
    "        return 0\n",
    "    elif \"Neutral\" in annotation or \"neutral\" in annotation:\n",
    "        return 1\n",
    "    elif \"Positiv\" in annotation or \"positiv\" in annotation:\n",
    "        return 2\n",
    "    else:\n",
    "        return None  # In case of unexpected values\n",
    "\n",
    "data[\"label\"] = data[\"label\"].map(map_sentiment_to_int)\n",
    "data[\"llm_annotation\"] = data[\"llm_annotation\"].map(map_sentiment_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(n=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selective sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bins and labels for confidence intervals\n",
    "bins = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "labels = [\"0-0.1\", \"0.1-0.2\", \"0.2-0.3\", \"0.3-0.4\", \"0.4-0.5\", \"0.5-0.6\", \"0.6-0.7\", \"0.7-0.8\", \"0.8-0.9\", \"0.9-1.0\"]\n",
    "data[\"confidence_interval\"] = pd.cut(data[\"confidence_scores\"], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "# Determine if LLM annotation is correct\n",
    "data[\"is_correct\"] = data[\"llm_annotation\"] == data[\"label\"]\n",
    "\n",
    "# Calculate the distribution of examples across intervals and correctness in the full dataset\n",
    "full_distribution = data.groupby([\"confidence_interval\", \"is_correct\"]).size() / len(data)\n",
    "\n",
    "# Calculate target sample sizes based on this distribution\n",
    "total_samples = 300\n",
    "sample_sizes = (full_distribution * total_samples).round().astype(int)\n",
    "\n",
    "# Initialize an empty DataFrame for the sampled data\n",
    "sampled_df = pd.DataFrame()\n",
    "\n",
    "# Sample 300 examples based on the calculated distribution\n",
    "for (interval, correct), size in sample_sizes.items():\n",
    "    # Only sample if the size is greater than zero\n",
    "    if size > 0:\n",
    "        # Get the subset of data for this interval and correctness\n",
    "        group_data = data[(data[\"confidence_interval\"] == interval) & (data[\"is_correct\"] == correct)]\n",
    "        \n",
    "        # Set `replace=True` only if we need more samples than available in this group\n",
    "        replace = size > len(group_data)\n",
    "        \n",
    "        # Sample the data\n",
    "        sampled_group = resample(\n",
    "            group_data,\n",
    "            n_samples=size,\n",
    "            random_state=42,\n",
    "            replace=replace\n",
    "        )\n",
    "        sampled_df = pd.concat([sampled_df, sampled_group])\n",
    "\n",
    "# Reset index for the sampled DataFrame\n",
    "sampled_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# save df to csv\n",
    "sampled_df.to_csv(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format fine-tuning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data = pd.read_csv(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create fine-tuning dataset with best-performing prompt and with simplest prompt\n",
    "\n",
    "messages = []\n",
    "for id, text in sampled_data.text.items():\n",
    "    label = {\"sentiment\": data.label[id]}\n",
    "    label = json.dumps(label)\n",
    "    article = {\n",
    "    \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"\"\"Du er en gennemsnitlig dansk nyhedsforbruger. Du får en overskrift og underoverskrift på en artikel, og skal tildele den en kategori svarende til det sentiment den fremkalder. Kategorier: ”Positiv”: Fremkalder en overordnet positiv sentiment. ”Negativ”: Fremkalder en overordnet negativ sentiment. ”Neutral”: Fremkalder hverken en positiv eller negativ sentiment. Giv et præcist svar i json: {{sentiment: ”kategori”}}.\"\"\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Artikel: {text} \\nArtiklen fremkalder dette sentiment:\"},\n",
    "            {\"role\": \"assistant\", \"content\": label}\n",
    "            ],\n",
    "    }\n",
    "\n",
    "    messages.append(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to json\n",
    "with open(\"\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "    for el in messages:\n",
    "        json_file.write(json.dumps(el, ensure_ascii=False))\n",
    "        json_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation data\n",
    "evaluation_data = pd.read_csv(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts\n",
    "sentiment_v1_prompt = \"\"\"\n",
    "Du er en gennemsnitlig dansk nyhedsforbruger. Du får en overskrift og underoverskrift på en artikel, og skal tildele den en kategori svarende til det sentiment den fremkalder.\n",
    "Kategorier: ”Positiv”: Fremkalder en overordnet positiv sentiment. ”Negativ”: Fremkalder en overordnet negativ sentiment. ”Neutral”: Fremkalder hverken en positiv eller negativ sentiment\n",
    "Giv et præcist svar i json: {{sentiment: ”kategori”}}.\n",
    "\"\"\"\n",
    "\n",
    "sentiment_v0_prompt = \"\"\"\n",
    "Du er en gennemsnitlig dansk nyhedsforbruger. Du får en overskrift og underoverskrift på en artikel, og skal tildele den en kategori svarende til det sentiment den fremkalder. \n",
    "Kategorier: ”Positiv”, ”Negativ”, ”Neutral”. \n",
    "Giv et præcist svar i json: {{sentiment: ”kategori”}}.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeroshot_sentiment_annotation(text, prompt):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"\", # specify fine-tuned model\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\", \n",
    "                    \"content\": f\"{prompt}\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": f\"Artikel: {text} \\nArtiklen fremkalder dette sentiment:\"\n",
    "                }\n",
    "            ],\n",
    "            temperature=0,\n",
    "            response_format={\n",
    "                \"type\": \"json_schema\",\n",
    "                \"json_schema\": {\n",
    "                    \"name\": \"topic_schema\",\n",
    "                    \"schema\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"sentiment\": {\n",
    "                                \"description\": \"Sentiment of the article\",\n",
    "                                \"type\": \"string\"\n",
    "                            },\n",
    "                            \"additionalProperties\": False\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Extract  response\n",
    "        sentiment = response.choices[0].message.content\n",
    "        return sentiment\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "    \n",
    "# Apply function and add results to df\n",
    "evaluation_data[\"llm_annotation\"] = evaluation_data[\"text\"].apply(lambda text: zeroshot_sentiment_annotation(text, prompt=sentiment_v0_prompt))\n",
    "\n",
    "# Save the results to csv file\n",
    "evaluation_data.to_csv(\"\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
